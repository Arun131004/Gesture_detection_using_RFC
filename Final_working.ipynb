{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "548bd3ea-2dcc-4f0b-b7ed-f7476868d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b09f998-9b03-4870-889d-6feaa07a61f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for class: hello\n",
      "Collecting data for class: okay\n",
      "Collecting data for class: no\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Collection\n",
    "DATA_DIR = './data'\n",
    "dataset_size = 100  # Number of images per gesture\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Open a GUI window to get the label\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "    label = simpledialog.askstring(\"Input\", \"Enter the label for this gesture (or type 'exit' to quit):\")\n",
    "    \n",
    "    if label is None or label.lower() == 'exit':\n",
    "        break  # Exit if the user closes the dialog or types \"exit\"\n",
    "\n",
    "    class_dir = os.path.join(DATA_DIR, label)\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "    \n",
    "    print(f'Collecting data for class: {label}')\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.putText(frame, f'Ready for {label}? Press \"Q\"!', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(25) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    counter = 0\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.imwrite(os.path.join(class_dir, f'{counter}.jpg'), frame)\n",
    "        counter += 1\n",
    "        cv2.waitKey(25)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1b461d-2a91-464c-a6d8-a6c02b998bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "data, labels = [], []\n",
    "\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux, x_, y_ = [], [], []\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x_.append(hand_landmarks.landmark[i].x)\n",
    "                    y_.append(hand_landmarks.landmark[i].y)\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    data_aux.append(hand_landmarks.landmark[i].x - min(x_))\n",
    "                    data_aux.append(hand_landmarks.landmark[i].y - min(y_))\n",
    "            data.append(data_aux)\n",
    "            labels.append(dir_)  # Store the label as a string (e.g., \"A\", \"B\", \"C\")\n",
    "\n",
    "# Save the dataset\n",
    "pickle.dump({'data': data, 'labels': labels}, open('data.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea0850b-7ab0-470d-ad69-df27835a74bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of samples were classified correctly!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Model Training\n",
    "data_dict = pickle.load(open('data.pickle', 'rb'))\n",
    "data, labels = np.asarray(data_dict['data']), np.asarray(data_dict['labels'])\n",
    "\n",
    "# Ensure labels remain as strings\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "print(f'{accuracy_score(y_predict, y_test) * 100:.2f}% of samples were classified correctly!')\n",
    "\n",
    "# Save the trained model\n",
    "pickle.dump({'model': model}, open('model.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d0cb01-2c35-45b6-9d4e-21c3a54cf970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay no no no no no no no no no no no no no okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello no no okay okay okay okay okay okay okay okay okay okay okay okay no no no no no no no no no no no no no no no Hello Hello Hello Hello Hello Hello Hello Hello Hello no no no no no no no no no okay okay okay Hello Hello Hello Hello no no no no no no no no okay okay okay okay okay okay okay okay okay no no no no no no no no no no no no no no no no no no okay okay okay okay no no no no no no no no no no no no no no no no no no no no no Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello no no no no no no no no no no no no no no no no no no okay okay okay Hello no no no no Hello Hello Hello Hello Hello Hello no no no no no no no okay okay okay okay okay okay okay okay okay no no no no no no no no no no no no no no no Hello Hello no no no no no no okay okay okay okay okay okay okay Hello Hello Hello Hello Hello no no no no okay okay okay okay okay okay no no no no no no no no no no no no Hello Hello Hello no no no no no no no no no no no no no no no no no no no no no no no okay okay okay okay okay okay okay Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello Hello no no no no no no no no no no no no no no no Hello Hello Hello Hello Hello Hello Hello Hello Hello no no no no no no okay okay okay okay okay okay "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m H, W, _ \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     14\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(frame_rgb)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hand_landmarks \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 4: Live Gesture Recognition\n",
    "model_dict = pickle.load(open('model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    data_aux, x_, y_ = [], [], []\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x_.append(hand_landmarks.landmark[i].x)\n",
    "                y_.append(hand_landmarks.landmark[i].y)\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                data_aux.append(hand_landmarks.landmark[i].x - min(x_))\n",
    "                data_aux.append(hand_landmarks.landmark[i].y - min(y_))\n",
    "\n",
    "        x1, y1 = int(min(x_) * W) - 10, int(min(y_) * H) - 10\n",
    "        x2, y2 = int(max(x_) * W) - 10, int(max(y_) * H) - 10\n",
    "\n",
    "        # Predict the gesture\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        # Display prediction on the screen\n",
    "        predicted_label = prediction[0]  # Directly using predicted string label\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3)\n",
    "        print(predicted_label,end=\" \")\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d7fcd-0b98-4cf6-a0e7-1bfc2d18853a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
